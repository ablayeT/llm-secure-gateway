from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from app.security import PromptFirewall # Import du module

# Initialisation de l'application
app = FastAPI(
    title="LLM Secure Gateway",
    description="Pare-feu pour filtrer les injections de prompt",
    version="1.0.0"
)

# Initialisation du moteur de s√©curit√©
firewall = PromptFirewall()

class PromptRequest(BaseModel):
    user_input: str
    metadata: dict | None = None

@app.get("/")
def health_check():
    return {"status": "running", "service": "LLM Firewall"}

@app.post("/analyze")
def analyze_prompt(request: PromptRequest):
    """
    Analyse le prompt entrant via le Firewall.
    Bloque la requ√™te si une injection est d√©tect√©e.
    """
    # 1. Scan du prompt
    analysis = firewall.scan(request.user_input)

    # 2. Prise de d√©cision
    if not analysis["is_safe"]:
        # LOG DE S√âCURIT√â (Tr√®s important pour un Admin Sys !)
        print(f"üö® ALERT: Attaque bloqu√©e ! Input: '{request.user_input}' - Raison: {analysis['reason']}")
        
        # Renvoit d'une erreur 403 (Forbidden)
        raise HTTPException(status_code=403, detail=analysis["reason"])

    # 3. Si c'est safe donc validation
    return {
        "status": "allowed",
        "message": "Prompt valid√© et s√©curis√©.",
        "original_input": request.user_input
    }